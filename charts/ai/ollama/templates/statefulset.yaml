apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ .Release.Name }}
  labels:
    kasten/backup: "true"
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  revisionHistoryLimit: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/controller: main
      app.kubernetes.io/name: {{ .Release.Name }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  serviceName: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}
        app.kubernetes.io/controller: main
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/name: {{ .Release.Name }}
    spec:
      enableServiceLinks: false
      serviceAccountName: {{ .Release.Name }}
      automountServiceAccountToken: true
      securityContext:
        runAsUser: 1000
        runAsGroup: 150
        runAsNonRoot: true
        fsGroup: 150
      containers:
        - env:
          - name: TZ
            value: {{ .Values.cluster.timezone }}
          - name: OLLAMA_HOST
            value: "0.0.0.0:11434"
          {{ range $key, $value := .Values.pods.main.env }}- name: {{ $key }}
            value: {{ $value | quote }}
          {{ end }}
          image: {{ .Values.pods.main.image.repository }}:{{ .Values.pods.main.image.tag }}
          imagePullPolicy: IfNotPresent
          name: main
          ports:
          - containerPort: {{ .Values.application.port }}
            name: http
            protocol: TCP
          volumeMounts:
          - mountPath: /.ollama
            name: data
          {{- if .Values.pods.main.gpu.enabled }}
          resources:
            requests:
              cpu: {{ .Values.pods.main.resources.requests.cpu }}
              memory: {{ .Values.pods.main.resources.requests.memory }}
              nvidia.com/gpu: {{ .Values.pods.main.gpu.count }}
            limits:
              cpu: {{ .Values.pods.main.resources.limits.cpu }}
              memory: {{ .Values.pods.main.resources.limits.memory }}
              nvidia.com/gpu: {{ .Values.pods.main.gpu.count }}
          {{- else }}
          resources:
            requests:
              cpu: {{ .Values.pods.main.resources.requests.cpu }}
              memory: {{ .Values.pods.main.resources.requests.memory }}
            limits:
              cpu: {{ .Values.pods.main.resources.limits.cpu }}
              memory: {{ .Values.pods.main.resources.limits.memory }}
          {{- end }}
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
          {{- if or .Values.ollama.models.pull .Values.ollama.models.run .Values.ollama.models.create }}
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    while ! /bin/ollama ps > /dev/null 2>&1; do
                      sleep 5
                    done

                    {{- $allModels := list -}}

                    {{- if .Values.ollama.models.pull }}
                    {{- range .Values.ollama.models.pull }}

                    {{- if contains ":" . }}
                      {{- $allModels = append $allModels . }}
                    {{- else }}
                      {{- $allModels = append $allModels (printf "%s:latest" .) }}
                    {{- end }}

                    /bin/ollama pull {{ternary "--insecure" "" $.Values.ollama.insecure | toString }} {{ . }}
                    {{- end }}
                    {{- end }}

                    {{- if .Values.ollama.models.create }}
                    {{- range .Values.ollama.models.create }}
                    {{- $allModels = append $allModels .name }}
                    {{- if .template }}
                    cat <<EOF > {{ include "ollama.modelsMountPath" $ }}/{{ .name }}
                    {{- .template | nindent 20 }}
                    EOF
                    /bin/ollama create {{ .name }} -f {{ include "ollama.modelsMountPath" $ }}/{{ .name }}
                    {{- end }}
                    {{- if .configMapRef }}
                    /bin/ollama create {{ .name }} -f /models/{{ .name }}
                    {{- end }}
                    {{- end }}
                    {{- end }}

                    {{- if .Values.ollama.models.run }}
                    {{- range .Values.ollama.models.run }}

                    {{- if contains ":" . }}
                      {{- $allModels = append $allModels . }}
                    {{- else }}
                      {{- $allModels = append $allModels (printf "%s:latest" .) }}
                    {{- end }}

                    /bin/ollama run {{ . }}
                    {{- end }}
                    {{- end }}

                    {{- if .Values.ollama.models.clean }}
                    /bin/ollama list | awk 'NR>1 {print $1}' | while read model; do
                      echo "{{ $allModels | join " " }}" | tr ' ' '\n' | grep -Fqx "$model" || /bin/ollama rm "$model"
                    done
                    {{- end }}
          {{- end }}
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: {{ .Release.Name }}-data
